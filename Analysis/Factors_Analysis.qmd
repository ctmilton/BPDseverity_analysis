---
title: "Analysis of Factors Data"
format: html
editor: visual
jupyter: python3
---

```{python}
import math
import statistics
import numpy as np
import pandas as pd
import scipy.stats
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import statsmodels.formula.api as smf
import seaborn as sns
sns.set(style="white")
sns.set(style="whitegrid", color_codes=True)
```


```{python}
f_df = pd.read_csv(r"../Data/FactorsData/factors.csv")
```

# Descriptive Statistics
```{python}
# more complicated code to get descriptive stats

# summary_paranoia = f_df.groupby("run", as_index=False)["paranoiaM"].agg(["count","min","max","mean","std"])
# print("Summary for paranoia\n",summary_paranoia)
# print()
# print()
# 
# summary_bdi = f_df.groupby("run", as_index=False)["bdiM"].agg(["count","min","max","mean","std"])
# print("Summary for bdi\n",summary_bdi)
# print()
# print()
# 
# summary_stais = f_df.groupby("run", as_index=False)["staisM"].agg(["count","min","max","mean","std"])
# print("Summary for stais\n",summary_stais)
# print()
# print()
# 
# summary_stait = f_df.groupby("run", as_index=False)["staitM"].agg(["count","min","max","mean","std"])
# print("Summary for stais\n",summary_stait)
# print()
# print()
```

```{python}
# simpler code to get descriptive statistics
result = f_df.iloc[:,f_df.columns.get_loc('paranoiaM'):f_df.columns.get_loc('staitM')+1].describe()
print(result)
```

# Condition Checking

## Distribution of Data
We will take a look at the distribution for the correlation tests and to better understand the data. We don't need to have normal distributions for logistic regressions.


```{python}
#| message: false

fig, ax = plt.subplots()
ax.boxplot((f_df.paranoiaM,f_df.bdiM,f_df.staisM,f_df.staitM),
            vert=False, showmeans=True, meanline=True,
            labels=('Paranoia', 'BDI', 'Stais', 'Stait'),
            patch_artist=True,
            medianprops={'linewidth': 2, 'color': 'purple'},
            meanprops={'linewidth': 2, 'color': 'red'})
plt.show()
```
There are potential outliers that may affect our models later. However, they are not too far away so I will keep them.


```{python}
plt.clf()
fig, ax = plt.subplots()
ax.hist(f_df.paranoiaM, 5, cumulative=False)
ax.set_xlabel('Paranoia')
ax.set_ylabel('Frequency')
plt.show()
```
`Paranoia` has a right skewed distribution.

```{python}
plt.clf()
fig, ax = plt.subplots()
ax.hist(f_df.bdiM, 8, cumulative=False)
ax.set_xlabel('BDI')
ax.set_ylabel('Frequency')
plt.show()
```
`BDI` has a right skewed distribution.

```{python}
plt.clf()
fig, ax = plt.subplots()
ax.hist(f_df.staisM, 8, cumulative=False)
ax.set_xlabel('Stais')
ax.set_ylabel('Frequency')
plt.show()
```
`Stais` has a right skewed distribution.

```{python}
plt.clf()
fig, ax = plt.subplots()
ax.hist(f_df.staitM, 8, cumulative=False)
ax.set_xlabel('Stait')
ax.set_ylabel('Frequency')
plt.show()
```
`Stait` has a non-normal distribution.

None of the distributions are normal.


## Correlation Among Factors
Since the distributions are non-normal, we use the non-parametric test, Spearman's Rank Correlation Test.

```{python}
corr = f_df.iloc[:,f_df.columns.get_loc('paranoiaM'):f_df.columns.get_loc('staitM')+1].corr(method='spearman')

print("Spearman's Correlation Matrix")
print(corr)
```
As seen in the correlation matrix, all factors are very strongly correlated with each other. Therefore, we will create logistic models for each individual factor later.

## Linearity of Log Odds
```{python}
# no longer converting groupn to (binary) categorical variable
#f_df['groupn'] = f_df['groupn'].astype('category')
#f_df['gendern'] = f_df['gendern'].astype('category')
print(f_df.dtypes)
```

```{python}
plt.clf()
paranoia_check = sns.regplot(x='paranoiaM', y='groupn', data=f_df,
                 logistic=True).set_title("Paranoia Log Odds Linear Plot")

paranoia_check.figure.savefig("./LogOddsPlots/paranoia_log_lin.png")
```

`Paranoia` passes the linearity of log odds check!

```{python}
#print(pd.isna(f_df['bdiM']))
#print(f_df['bdiM'].dtypes)

plt.clf()
bdi_check = sns.regplot(x='bdiM', y='groupn', data=f_df,
            logistic=True).set_title("BDI Log Odds Linear Plot")

#bdi_check.figure.savefig("./LogOddsPlots/bdi_log_lin.png")
```
Somehow there is some error here but I'm not sure why. From what we can see, I believe that this should be fine.

```{python}
plt.clf()
stais_check = sns.regplot(x='staisM', y='groupn', data=f_df,
              logistic=True).set_title("State Anxiety Log Odds Linear Plot")

stais_check.figure.savefig("./LogOddsPlots/stais_log_lin.png")
```

`Stais` passes the linearity of log odds check!

```{python}
plt.clf()
stait_check = sns.regplot(x='staitM', y='groupn', data=f_df,
              logistic=True).set_title("Trait Anxiety Log Odds Linear Plot")

stait_check.figure.savefig("./LogOddsPlots/stait_log_lin.png")
```

`Stait` passes the linearity of log odds check!

# Logistic Regressions
We control for the confounding variable `gender`.

## Paranoia Logistic Model
```{python}
p_model= smf.logit(formula="groupn~paranoiaM+gendern", data=f_df).fit()
p_model.summary()
```
Since the p-value for `paranoia` is less than alpha=0.05, we see that `paranoia` is a significant predictor for BPD diagnosis. The p-value for `gender` is much greater than 0.05, so it seems that gender does not have a significant influence on the diagnosis of BPD when in conjunction with `paranoia`.

### Interpreting with Odd Ratios
```{python}
# getting the odd ratios, z-value, and 95% CI
pmodel_odds = pd.DataFrame(np.exp(p_model.params), columns= ['OR'])
pmodel_odds['z-value']= p_model.pvalues
pmodel_odds[['2.5%', '97.5%']] = np.exp(p_model.conf_int())
pmodel_odds
```
With every one unit increase in `paranoia score`, the odds of being diagnosed with BPD increases by a factor of about 1.229 [in females and about 1.655 in males; probably don't need that statement].


## BDI Logistic Model
```{python}
bdi_model= smf.logit(formula="groupn~bdiM", data=f_df).fit()
#bdi_model.summary()
```

## Stais Logistic Model

## Stait Logistic Model


# Evaluating the Models
